{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import distutils.version\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from utils import save_checkpoint, load_checkpoint, print_examples\n",
    "from get_loader import get_loader\n",
    "from model import CNNtoRNN\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root: /SSD/ne6101157/pac4_mini/train\n",
      "Number of videos: 512\n",
      "Number of rules: 512\n",
      "root: /SSD/ne6101157/pac4_mini/test\n",
      "Number of videos: 162\n",
      "Number of rules: 162\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_loader, train_dataset = get_loader(\n",
    "    root_folder=\"/SSD/ne6101157/pac4_mini/train\",\n",
    "    transform=None,\n",
    "    num_workers=2,\n",
    ")\n",
    "test_loader, test_dataset = get_loader(\n",
    "    root_folder=\"/SSD/ne6101157/pac4_mini/test\",\n",
    "    transform=None,\n",
    "    batch_size=1,\n",
    "    num_workers=2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "load_model = False\n",
    "save_model = True\n",
    "train_CNN = False\n",
    "\n",
    "# Hyperparameters\n",
    "embed_size = 256\n",
    "hidden_size = 256\n",
    "vocab_size = len(train_dataset.vocab)\n",
    "num_layers = 3\n",
    "learning_rate = 3e-4\n",
    "num_epochs = 30\n",
    "\n",
    "resume_epoch = 0\n",
    "save_dir_root = os.path.join(\"/SSD/ne6101157/image_captioning/modelpath\")\n",
    "if resume_epoch != 0:\n",
    "    runs = sorted(glob.glob(os.path.join(save_dir_root, 'run', 'run_*')))\n",
    "    run_id = int(runs[-1].split('_')[-1]) if runs else 0\n",
    "else:\n",
    "    runs = sorted(glob.glob(os.path.join(save_dir_root, 'run', 'run_*')))\n",
    "    run_id = int(runs[-1].split('_')[-1]) + 1 if runs else 0\n",
    "save_dir = os.path.join(save_dir_root, 'run', 'run_' + str(run_id))\n",
    "load_dir = os.path.join(save_dir_root, 'run', 'run_' + str(run_id-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight\n",
      "torch.Size([64, 1, 3, 3, 3])\n",
      "conv1.bias\n",
      "torch.Size([64])\n",
      "bn1.weight\n",
      "torch.Size([64])\n",
      "bn1.bias\n",
      "torch.Size([64])\n",
      "bn1.running_mean\n",
      "torch.Size([64])\n",
      "bn1.running_var\n",
      "torch.Size([64])\n",
      "bn1.num_batches_tracked\n",
      "torch.Size([])\n",
      "conv2.weight\n",
      "torch.Size([128, 64, 3, 3, 3])\n",
      "conv2.bias\n",
      "torch.Size([128])\n",
      "bn2.weight\n",
      "torch.Size([128])\n",
      "bn2.bias\n",
      "torch.Size([128])\n",
      "bn2.running_mean\n",
      "torch.Size([128])\n",
      "bn2.running_var\n",
      "torch.Size([128])\n",
      "bn2.num_batches_tracked\n",
      "torch.Size([])\n",
      "conv3a.weight\n",
      "torch.Size([256, 128, 3, 3, 3])\n",
      "conv3a.bias\n",
      "torch.Size([256])\n",
      "conv3b.weight\n",
      "torch.Size([256, 256, 3, 3, 3])\n",
      "conv3b.bias\n",
      "torch.Size([256])\n",
      "bn3.weight\n",
      "torch.Size([256])\n",
      "bn3.bias\n",
      "torch.Size([256])\n",
      "bn3.running_mean\n",
      "torch.Size([256])\n",
      "bn3.running_var\n",
      "torch.Size([256])\n",
      "bn3.num_batches_tracked\n",
      "torch.Size([])\n",
      "conv4a.weight\n",
      "torch.Size([512, 256, 3, 3, 3])\n",
      "conv4a.bias\n",
      "torch.Size([512])\n",
      "conv4b.weight\n",
      "torch.Size([512, 512, 3, 3, 3])\n",
      "conv4b.bias\n",
      "torch.Size([512])\n",
      "bn4.weight\n",
      "torch.Size([512])\n",
      "bn4.bias\n",
      "torch.Size([512])\n",
      "bn4.running_mean\n",
      "torch.Size([512])\n",
      "bn4.running_var\n",
      "torch.Size([512])\n",
      "bn4.num_batches_tracked\n",
      "torch.Size([])\n",
      "conv5a.weight\n",
      "torch.Size([512, 512, 3, 3, 3])\n",
      "conv5a.bias\n",
      "torch.Size([512])\n",
      "conv5b.weight\n",
      "torch.Size([512, 512, 3, 3, 3])\n",
      "conv5b.bias\n",
      "torch.Size([512])\n",
      "bn5.weight\n",
      "torch.Size([512])\n",
      "bn5.bias\n",
      "torch.Size([512])\n",
      "bn5.running_mean\n",
      "torch.Size([512])\n",
      "bn5.running_var\n",
      "torch.Size([512])\n",
      "bn5.num_batches_tracked\n",
      "torch.Size([])\n",
      "conv6.weight\n",
      "torch.Size([256, 32768, 1, 1])\n",
      "conv6.bias\n",
      "torch.Size([256])\n",
      "fc6.weight\n",
      "torch.Size([256, 512])\n",
      "fc6.bias\n",
      "torch.Size([256])\n",
      "fc7.weight\n",
      "torch.Size([128, 256])\n",
      "fc7.bias\n",
      "torch.Size([128])\n",
      "fc8.weight\n",
      "torch.Size([4, 128])\n",
      "fc8.bias\n",
      "torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# for tensorboard\n",
    "writer = SummaryWriter(\"runs/pac4\")\n",
    "step = 0\n",
    "\n",
    "classifier = \"C3D\" # 'R3D' or 'R2Plus1D' or 'C3D' or 'PacR3D'\n",
    "# initialize model, loss etc\n",
    "model = CNNtoRNN(embed_size, hidden_size, vocab_size, num_layers, classifier).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=train_dataset.vocab.stoi[\"<PAD>\"])\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "if classifier == \"R3D\":\n",
    "    pretrained_R3D_weights = torch.load('/SSD/ne6101157/image_captioning/modelpath/run/run_10/models/R3D-pac4_epoch-19.pth.tar')\n",
    "    model.encoderCNN.R3D.load_state_dict(pretrained_R3D_weights['state_dict'])\n",
    "    for param in model.encoderCNN.R3D.parameters():\n",
    "        param.requires_grad = False\n",
    "elif classifier == \"R2Plus1D\":\n",
    "    pretrained_R2Plus1D_weights = torch.load('/SSD/ne6101157/image_captioning/modelpath/run/run_10/models/R2Plus1D-pac4_epoch-19.pth.tar')\n",
    "    model.encoderCNN.R2Plus1D.load_state_dict(pretrained_R2Plus1D_weights['state_dict'])\n",
    "    for param in model.encoderCNN.R2Plus1D.parameters():\n",
    "        param.requires_grad = False\n",
    "elif classifier == \"C3D\":\n",
    "    pretrained_C3D_weights = torch.load('/SSD/ne6101157/image_captioning/modelpath/run/run_10/models/C3D-pac4_epoch-19.pth.tar')\n",
    "    model.encoderCNN.C3D.load_state_dict(pretrained_C3D_weights['state_dict'])\n",
    "    for param in model.encoderCNN.C3D.parameters():\n",
    "        param.requires_grad = False\n",
    "elif classifier == \"PacR3D\":\n",
    "    pretrained_PacR3D_weights = torch.load('/SSD/ne6101157/image_captioning/modelpath/run/run_10/models/C3D-pac4_epoch-9.pth.tar')\n",
    "    model.encoderCNN.PacR3D.load_state_dict(pretrained_PacR3D_weights['state_dict'])\n",
    "    for param in model.encoderCNN.PacR3D.parameters():\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load model from /SSD/ne6101157/image_captioning/modelpath/run/run_9/models/epoch-29.pth.tar\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_name = os.path.join(load_dir, 'models', '_epoch-' + str(29) + '.pth.tar')\n",
    "print(\"Load model from {}\\n\".format(os.path.join(load_dir, 'models', 'epoch-' + str(29) + '.pth.tar')))\n",
    "            \n",
    "if load_model:\n",
    "    step = load_checkpoint(torch.load(file_name), model, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 37.92M\n",
      "0 / 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/128 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv3d(1, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "  (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (3): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "  (4): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (5): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (6): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "  (7): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "  (8): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (9): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (10): Conv3d(256, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "  (11): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "  (12): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (13): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (14): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "  (15): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "  (16): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (17): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=(0, 1, 1), dilation=1, ceil_mode=False)\n",
      "  (18): Conv2d(32768, 256, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1))\n",
      "  (19): AdaptiveAvgPool3d(output_size=8)\n",
      "  (20): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (21): Linear(in_features=256, out_features=256, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 4-dimensional input for 4-dimensional weight [256, 32768, 1, 1], but got 5-dimensional input of size [4, 512, 32, 3, 3] instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_744972/540214474.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mcaptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcaptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         loss = criterion(\n\u001b[1;32m     28\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/C3D/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/SSD/ne6101157/image_captioning/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, video, captions)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mvideo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvideo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoderCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoderRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/C3D/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/SSD/ne6101157/image_captioning/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/C3D/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/C3D/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/C3D/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/C3D/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/C3D/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    441\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    442\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 443\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 4-dimensional input for 4-dimensional weight [256, 32768, 1, 1], but got 5-dimensional input of size [4, 512, 32, 3, 3] instead"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Total params: %.2fM' % (sum(p.numel() for p in model.parameters()) / 1000000.0))\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    #Uncomment the line below to see a couple of test cases\n",
    "    # print_examples(model, device, dataset)\n",
    "    print(epoch, \"/\", num_epochs)\n",
    "    if save_model:\n",
    "        \n",
    "        checkpoint = {\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"step\": step,\n",
    "        }\n",
    "        if epoch % 10 == (9):\n",
    "            file_name = os.path.join(save_dir, 'models', '_' + classifier +'_epoch-' + str(epoch) + '.pth.tar')\n",
    "            print(\"Save model at {}\\n\".format(os.path.join(save_dir, 'models', 'epoch-' + str(epoch) + '.pth.tar')))\n",
    "            save_checkpoint(checkpoint, filename=file_name)\n",
    "\n",
    "    for idx, (imgs, captions) in tqdm(\n",
    "        enumerate(train_loader), total=len(train_loader), leave=False\n",
    "    ):\n",
    "        imgs = imgs.to(device)\n",
    "        captions = captions.to(device)\n",
    "        \n",
    "        outputs = model(imgs, captions[:-1])\n",
    "        loss = criterion(\n",
    "            outputs.reshape(-1, outputs.shape[2]), captions.reshape(-1)\n",
    "        )\n",
    "\n",
    "        writer.add_scalar(\"Training loss\", loss.item(), global_step=step)\n",
    "        step += 1\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(loss)\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.eval()\n",
    "# validate with test data\n",
    "actual, predicted = list(), list()\n",
    "\n",
    "for idx, (imgs, captions) in tqdm(\n",
    "        enumerate(test_loader), total=len(test_loader), leave=False\n",
    "    ):\n",
    "    imgs = imgs.to(device)\n",
    "    # predict the caption for image\n",
    "    y_pred = model.caption_video(imgs, test_dataset.vocab)\n",
    "    # split into words\n",
    "    actual_captions = [test_dataset.vocab.itos[int(caption)] for caption in captions]\n",
    "    # append to the list\n",
    "    actual.append(actual_captions)\n",
    "    predicted.append(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "# 創建ROUGE評估器\n",
    "rouge_evaluator = Rouge()\n",
    "candidates = []\n",
    "references = []\n",
    "for i, txt in enumerate(predicted):\n",
    "    candidates.append(' '.join(txt))\n",
    "for i, txt in enumerate(actual):\n",
    "    references.append(' '.join(txt))\n",
    "# 計算ROUGE分數\n",
    "rouge_scores = rouge_evaluator.get_scores(candidates, references)\n",
    "\n",
    "# Initialize variables for accumulating scores\n",
    "total_rouge_1_score = 0\n",
    "total_rouge_2_score = 0\n",
    "total_rouge_l_score = 0\n",
    "total_rouge_1_p = 0\n",
    "total_rouge_2_p = 0\n",
    "total_rouge_l_p = 0\n",
    "total_rouge_1_r = 0\n",
    "total_rouge_2_r = 0\n",
    "total_rouge_l_r = 0\n",
    "# Accumulate scores for all candidate-reference pairs\n",
    "for i in range(len(candidates)):\n",
    "    rouge_1_score = rouge_scores[i]['rouge-1']['f']\n",
    "    rouge_1_p = rouge_scores[i]['rouge-1']['p']\n",
    "    rouge_1_r = rouge_scores[i]['rouge-1']['r']\n",
    "    rouge_2_score = rouge_scores[i]['rouge-2']['f']\n",
    "    rouge_2_p = rouge_scores[i]['rouge-2']['p']\n",
    "    rouge_2_r = rouge_scores[i]['rouge-2']['r']\n",
    "    rouge_l_score = rouge_scores[i]['rouge-l']['f']\n",
    "    rouge_l_p = rouge_scores[i]['rouge-l']['p']\n",
    "    rouge_l_r = rouge_scores[i]['rouge-l']['r']\n",
    "    total_rouge_1_score += rouge_1_score\n",
    "    total_rouge_2_score += rouge_2_score\n",
    "    total_rouge_l_score += rouge_l_score\n",
    "    total_rouge_1_p += rouge_1_p\n",
    "    total_rouge_2_p += rouge_2_p\n",
    "    total_rouge_l_p += rouge_l_p\n",
    "    total_rouge_1_r += rouge_1_r\n",
    "    total_rouge_2_r += rouge_2_r\n",
    "    total_rouge_l_r += rouge_l_r\n",
    "\n",
    "# Compute average scores\n",
    "avg_rouge_1_score = total_rouge_1_score / len(candidates)\n",
    "avg_rouge_2_score = total_rouge_2_score / len(candidates)\n",
    "avg_rouge_l_score = total_rouge_l_score / len(candidates)\n",
    "avg_rouge_1_p = total_rouge_1_p / len(candidates)\n",
    "avg_rouge_2_p = total_rouge_2_p / len(candidates)\n",
    "avg_rouge_l_p = total_rouge_l_p / len(candidates)\n",
    "avg_rouge_1_r = total_rouge_1_r / len(candidates)\n",
    "avg_rouge_2_r = total_rouge_2_r / len(candidates)\n",
    "avg_rouge_l_r = total_rouge_l_r / len(candidates)\n",
    "# Print average scores\n",
    "print(\"Average ROUGE-1 score:\", avg_rouge_1_score)\n",
    "print(\"Average ROUGE-2 score:\", avg_rouge_2_score)\n",
    "print(\"Average ROUGE-L score:\", avg_rouge_l_score)\n",
    "# Print average scores\n",
    "print(\"Average ROUGE-1 precision:\", avg_rouge_1_p)\n",
    "print(\"Average ROUGE-2 precision:\", avg_rouge_2_p)\n",
    "print(\"Average ROUGE-L precision:\", avg_rouge_l_p)\n",
    "# Print average scores\n",
    "print(\"Average ROUGE-1 recall:\", avg_rouge_1_r)\n",
    "print(\"Average ROUGE-2 recall:\", avg_rouge_2_r)\n",
    "print(\"Average ROUGE-L recall:\", avg_rouge_l_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "# Calculate BLEU-1 score\n",
    "bleu_1_score = corpus_bleu(actual, predicted, weights=(1, 0, 0, 0))\n",
    "\n",
    "# Print BLEU-1 score\n",
    "print(\"BLEU-1 score:\", bleu_1_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
