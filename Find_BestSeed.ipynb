{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------training_seed_0---------------------\n",
      "Training model on CIC-IDS2018-ZSL-v1-DoS dataset...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3575452/1515139725.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;31m########################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training model on {} dataset...'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0mtrain_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVideoDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclip_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m     \u001b[0mtest_dataloader\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVideoDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclip_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/yulin_code/dataloaders/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, split, dataset, clip_len, preprocess)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_width\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_integrity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m     \u001b[0;31m# 檢查路徑是否存在\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             raise RuntimeError('Dataset not found or corrupted.' +\n\u001b[1;32m     36\u001b[0m                                ' You need to download it from official website.')\n",
      "\u001b[0;32m~/yulin_code/dataloaders/dataset.py\u001b[0m in \u001b[0;36mcheck_integrity\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_integrity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/genericpath.py\u001b[0m in \u001b[0;36mexists\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;34m\"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: stat: path should be string, bytes, os.PathLike or integer, not NoneType"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "from datetime import datetime\n",
    "import socket\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch import nn, optim\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from dataloaders.dataset_ZSL import VideoDataset\n",
    "from network import R3D_model, C3D_model, R2Plus1D_model, Pac3D_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "for seed in range(51):\n",
    "    print(f\"---------------training_seed_{seed}---------------------\")\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) \n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # print(\"Device being used:\", device)\n",
    "\n",
    "    ############################\n",
    "    ####    Parameters      ####\n",
    "    ############################\n",
    "    nEpochs = 1  # Number of epochs for training\n",
    "    resume_epoch = 0  # Default is 0, change if want to resume\n",
    "    useTest = True # See evolution of the test set when training\n",
    "    nTestInterval = 5 # Run on test set every nTestInterval epochs\n",
    "    save_epoch = 10 # Store a model every save_epoch\n",
    "    lr = 1e-3 # Learning rate\n",
    "    clip_len = 256 # frames of each video\n",
    "\n",
    "    ###################################\n",
    "    ####    Options of Dataset     ####\n",
    "    ###################################\n",
    "    dataset = 'CIC-IDS2018-ZSL-v1-DoS' \n",
    "    modelName = 'Pac3D' \n",
    "    saveName = modelName + '-' + dataset\n",
    "\n",
    "    if dataset == 'CIC-IDS2018-all':\n",
    "        num_classes = 14\n",
    "    elif dataset == 'CIC-IDS2018-all-v1':\n",
    "        num_classes = 14\n",
    "    elif dataset == 'CIC-IDS2018-v1-DoS':\n",
    "        num_classes = 4\n",
    "    elif dataset == 'CIC-IDS2018-v1-DDoS':\n",
    "        num_classes = 3\n",
    "    elif dataset == 'CIC-IDS2018-v1-Auth':\n",
    "        num_classes = 2\n",
    "    elif dataset == 'CIC-IDS2018-v1-Web':\n",
    "        num_classes = 3\n",
    "    elif dataset == 'CIC-IDS2018-v1-Other':\n",
    "        num_classes = 2\n",
    "    elif dataset == 'CIC-IDS2018-v2-DoS':\n",
    "        num_classes = 4\n",
    "    elif dataset == 'CIC-IDS2018-v3-DoS':\n",
    "        num_classes = 4\n",
    "    elif dataset == 'CIC-IDS2018-v3-DDoS':\n",
    "        num_classes = 3\n",
    "    elif dataset == 'CIC-IDS2018-v3-Auth':\n",
    "        num_classes = 2\n",
    "    elif dataset == 'CIC-IDS2018-v3-Web':\n",
    "        num_classes = 3\n",
    "    elif dataset == 'CIC-IDS2018-v3-Other':\n",
    "        num_classes = 2\n",
    "    elif dataset == 'CIC-IDS2018-ZSL-v1-DDoS':\n",
    "        num_classes = 2\n",
    "    elif dataset == 'CIC-IDS2018-ZSL-v1-DoS':\n",
    "        num_classes = 3\n",
    "    elif dataset == 'CIC-IDS2018-ZSL-v1-Web':\n",
    "        num_classes = 2\n",
    "    else:\n",
    "        print('No Dataset')\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "    ######################################\n",
    "    ####   Load model & parameters    ####\n",
    "    ######################################\n",
    "    if modelName == 'C3D':\n",
    "        model = C3D_model.C3D(num_classes=num_classes, pretrained=False)\n",
    "        train_params = [{'params': C3D_model.get_1x_lr_params(model), 'lr': lr},\n",
    "                        {'params': C3D_model.get_10x_lr_params(model), 'lr': lr * 10}]\n",
    "    elif modelName == 'R2Plus1D':\n",
    "        model = R2Plus1D_model.R2Plus1DClassifier(num_classes=num_classes, layer_sizes=(2, 2, 2, 2))\n",
    "        train_params = [{'params': R2Plus1D_model.get_1x_lr_params(model), 'lr': lr},\n",
    "                        {'params': R2Plus1D_model.get_10x_lr_params(model), 'lr': lr * 10}]\n",
    "    elif modelName == 'R3D':\n",
    "        model = R3D_model.R3DClassifier(num_classes=num_classes, layer_sizes=(2, 2, 2, 2))\n",
    "        train_params = model.parameters()\n",
    "    elif modelName == 'Pac3D':\n",
    "        model = Pac3D_model.Pac3DClassifier(num_classes=num_classes, layer_sizes=(2, 2))\n",
    "        train_params = model.parameters()\n",
    "    else:\n",
    "        print('We only implemented C3D and R2Plus1D models.')\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "    ######################################\n",
    "    ####   Load model & parameters    ####\n",
    "    ######################################\n",
    "    criterion = nn.CrossEntropyLoss()  \n",
    "    optimizer = optim.Adam(train_params, lr=lr, weight_decay=5e-4)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)  # the scheduler divides the lr by 10 every 5 epochs\n",
    "    model.to(device)\n",
    "    criterion.to(device)\n",
    "\n",
    "    ########################\n",
    "    ####   Load Data    ####\n",
    "    ########################\n",
    "    print('Training model on {} dataset...'.format(dataset))\n",
    "    train_dataloader = DataLoader(VideoDataset(dataset=dataset, split='train', clip_len=clip_len), batch_size=16, shuffle=True, num_workers=0)\n",
    "    test_dataloader  = DataLoader(VideoDataset(dataset=dataset, split='test', clip_len=clip_len), batch_size=16, num_workers=0)\n",
    "\n",
    "    train_size = len(train_dataloader.dataset)\n",
    "    test_size = len(test_dataloader.dataset)\n",
    "\n",
    "\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    for epoch in range(resume_epoch, nEpochs):\n",
    "        start_time = timeit.default_timer()\n",
    "\n",
    "        # reset the running loss and corrects\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0.0\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        for inputs, labels in tqdm(train_dataloader):\n",
    "            # move inputs and labels to the device the training is taking place on\n",
    "            labels = labels.type(torch.LongTensor)\n",
    "            inputs = Variable(inputs, requires_grad=True).to(device)\n",
    "            labels = Variable(labels).to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            probs = nn.Softmax(dim=1)(outputs)\n",
    "            preds = torch.max(probs, 1)[1]\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        epoch_loss = running_loss / train_size\n",
    "        epoch_acc = running_corrects.double() / train_size\n",
    "\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accs.append(epoch_acc)\n",
    "            \n",
    "        print(\"[train] Epoch: {}/{} Loss: {} Acc: {}\".format(epoch+1, nEpochs, epoch_loss, epoch_acc))\n",
    "        stop_time = timeit.default_timer()\n",
    "\n",
    "        best_performanse_file = f\"Best_Performance/best_performance_{dataset}_new.txt\"\n",
    "        with open(best_performanse_file, 'a') as f:\n",
    "            f.write(f\"Seed: {seed} Best Performance: {epoch_acc}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
